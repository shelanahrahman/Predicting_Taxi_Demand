{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "# a nice way of filtering out deprecated warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Zone Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzone = pd.read_csv(\"../raw_data/large/taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NYC Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/lpgzfz8d1lv6zzph655b4mkm0000gn/T/ipykernel_21194/1612433812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2019\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../raw_data/feather/df2019.feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf20192\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../raw_data/feather/df20192.feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf20193\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../raw_data/feather/df20193.feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         return feather.read_feather(\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(source, columns, use_threads, memory_map)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0m_check_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     return (read_table(source, columns=columns, memory_map=memory_map)\n\u001b[0;32m--> 219\u001b[0;31m             .to_pandas(use_threads=use_threads))\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0;32m-> 1129\u001b[0;31m                                     list(extension_columns.keys()))\n\u001b[0m\u001b[1;32m   1130\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1131\u001b[0m             for item in result]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df2019 = pd.read_feather('../raw_data/feather/df2019.feather')\n",
    "df20192 = pd.read_feather('../raw_data/feather/df20192.feather')\n",
    "df20193 = pd.read_feather('../raw_data/feather/df20193.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code joins the data for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi = df2019.append(df20192, ignore_index = True)\n",
    "df_taxi = df_taxi.append(df20193, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory usage\n",
    "del df2019\n",
    "del df20192\n",
    "del df20193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size= len(df_taxi)\n",
    "old_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert datatypes to appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_taxi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, reduce memory usage by changing to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert to int8\n",
    "for i in ['VendorID', 'RatecodeID', 'passenger_count','payment_type','PULocationID', \n",
    "          'DOLocationID']:\n",
    "    df_taxi[i]= df_taxi[i].astype('int16')\n",
    "\n",
    "#convert to float32\n",
    "for i in [ 'trip_distance','fare_amount', \n",
    "          'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "          'improvement_surcharge', 'total_amount',\n",
    "          'congestion_surcharge']:\n",
    "    df_taxi[i]= df_taxi[i].astype('float32')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the .info(), the datetime columns have a data type of an object, convert it to a datatime date type in order to access more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['tpep_pickup_datetime']= pd.to_datetime(df_taxi['tpep_pickup_datetime'])\n",
    "print(\"converted pickup to \", type(df_taxi['tpep_pickup_datetime'][0]))\n",
    "df_taxi['tpep_dropoff_datetime']= pd.to_datetime(df_taxi['tpep_dropoff_datetime'])\n",
    "print(\"converted dropoff to \", type(df_taxi['tpep_dropoff_datetime'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi= df_taxi.drop(columns=['index', 'store_and_fwd_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_taxi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Intuitions based on columns:\n",
    "    \n",
    "    1. Date and Location feature will be relevant for the model that predicts taxi demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY: Sorted Taxi Data Types for further cleaning\n",
    "\n",
    "- Geographical data: PULocationID , DOLocationID\n",
    "- Datatime data: tpep_pickup_datetime, tpep_dropoff_datetime\n",
    "- Categorical Data: VendorID, RatecodeID, payment_type \n",
    "- Numerical Data: passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surchage, total_amount, congestion_surcharge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FHVHV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Feather\n",
    "dffhv2019 = pd.read_feather(\"../raw_data/feather/fhvhv_tripdata_2019-03.feather\")\n",
    "dffhv20192 = pd.read_feather(\"../raw_data/feather/fhvhv_tripdata_2019-04.feather\")\n",
    "dffhv20193 = pd.read_feather(\"../raw_data/feather/fhvhv_tripdata_2019-05.feather\")\n",
    "\n",
    "#The following code joins the data for each year\n",
    "\n",
    "df_fhv = dffhv2019.append(dffhv20192, ignore_index = True)\n",
    "df_fhv = df_fhv.append(dffhv20193, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory usage\n",
    "del dffhv2019\n",
    "del dffhv20192\n",
    "del dffhv20193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_fhv.shape)\n",
    "df_fhv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size_fhv= len(df_fhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Invalid Location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.loc[df_fhv['PULocationID']<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fhv[\"SR_Flag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if there is missing entries\n",
    "print(df_fhv.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_fhv[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_fhv[\"dispatching_base_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_fhv[\"hvfhs_license_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_fhv[\"SR_Flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since locationID is very relevant, the entries with missing locationID values should be removed. However, there is no need to remove any rows since there is no missing locationID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert datatypes to appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fhv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, reduce memory usage for running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#convert to Location IDs to int8\n",
    "for i in [\"PULocationID\", \"DOLocationID\"]:\n",
    "    df_fhv[i]= df_fhv[i].astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the .info(), the datetime columns have a data type of an object, convert it to a datatime date type in order to access more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fhv['pickup_datetime']= pd.to_datetime(df_fhv['pickup_datetime'])\n",
    "print(\"converted pickup to \", type(df_fhv['pickup_datetime'][0]))\n",
    "df_fhv['dropoff_datetime']=  pd.to_datetime(df_fhv['dropoff_datetime'], errors = 'coerce')\n",
    "print(\"converted dropoff to\", type(df_fhv['dropoff_datetime'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fhv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY: Sorted FHV Data Types for further cleaning\n",
    "- Geographical data: PULocationID , DOLocationID\n",
    "- Datatime data: pickup_datetime, dropoff_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data= ['VendorID', 'RatecodeID', 'payment_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the values align with the values given with the \n",
    "# data details\n",
    "for i in categorical_data:\n",
    "    print(i+ ':',df_taxi[i].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since VendorID is an irrelevant feature, we wouldn't need to remove the invalid ID of 4 since the rest of the features of the entries may be valid.\n",
    "For RatecodeID, there is an invalid RatecodeID of 99. Since we want only want standard rides to predict for typical demand, this should be removed along with the other payment types that are not standard.\n",
    "Before we remove these values, we should inspect the distribution of categorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_data:\n",
    "    sns.barplot(df_taxi[i].value_counts().index, df_taxi[i].value_counts())\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot tells us that majority of the data lies in the standard rate with payment mostly being through cash or card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi= df_taxi.loc[df_taxi[\"RatecodeID\"]==1]\n",
    "df_taxi=  df_taxi.loc[df_taxi[\"payment_type\"]==1] #remove payment 2 since we are looking at tips as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the data since they are irrelevant. See report for more information\n",
    "df_taxi= df_taxi.drop(columns= ['VendorID', 'RatecodeID', 'payment_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning DateTime Data for Taxi and FHV datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove invalid trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for and remove invalid trips where there was a pickup after the dropoff. There cannot be a trip where there is 0 seconds trip since there is a minimum of 2.50 fee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_taxi.loc[df_taxi['tpep_pickup_datetime']>= df_taxi['tpep_dropoff_datetime']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi= df_taxi.loc[df_taxi['tpep_pickup_datetime']< df_taxi['tpep_dropoff_datetime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FHV Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fhv.loc[df_fhv['pickup_datetime']>= df_fhv['dropoff_datetime']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv= df_fhv.loc[df_fhv['pickup_datetime']< df_fhv['dropoff_datetime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding DataTime related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating  total_trip_duration feature for Taxi and FHV\n",
    "This is a feature that consists of the total trip duration in minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to seconds\n",
    "df_taxi['total_trip_duration'] = (df_taxi['tpep_dropoff_datetime'] - df_taxi['tpep_pickup_datetime']).astype('timedelta64[s]')\n",
    "#convert to minutes\n",
    "df_taxi['total_trip_duration'] = df_taxi['total_trip_duration']/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to seconds\n",
    "df_fhv['total_trip_duration'] = (df_fhv['dropoff_datetime'] - df_fhv['pickup_datetime']).astype('timedelta64[s]')\n",
    "#convert to minutes\n",
    "df_fhv['total_trip_duration'] = df_fhv['total_trip_duration']/60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove datetime outside of range for Taxi and FHV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date1= pd.Timestamp(datetime(2019, 3, 1))\n",
    "date2= pd.Timestamp(datetime(2019, 5, 31))\n",
    "# taxi\n",
    "df_taxi= df_taxi.iloc[(df_taxi['tpep_pickup_datetime']).values> date1]\n",
    "df_taxi= df_taxi.iloc[(df_taxi['tpep_pickup_datetime']).values< date2]\n",
    "df_taxi= df_taxi.iloc[(df_taxi[\"tpep_dropoff_datetime\"]).values> date1]\n",
    "df_taxi= df_taxi.iloc[(df_taxi[\"tpep_dropoff_datetime\"]).values< date2]\n",
    "#fhv\n",
    "df_fhv= df_fhv.iloc[(df_fhv['pickup_datetime']).values> date1]\n",
    "df_fhv= df_fhv.iloc[(df_fhv['pickup_datetime']).values< date2]\n",
    "df_fhv= df_fhv.iloc[(df_fhv[\"dropoff_datetime\"]).values> date1]\n",
    "df_fhv= df_fhv.iloc[(df_fhv[\"dropoff_datetime\"]).values< date2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create avespeed_ mileshr feature for Taxi Data\n",
    " This feature demonstrates speed of miles per hour (which is consistent with standard US speed measure). Lower average speed may be associated with traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['avespeed_mileshr']= df_taxi['trip_distance'] / (df_taxi['total_trip_duration']/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time session feature for Taxi and FHV dataset\n",
    "Bin hours into time sessions\n",
    "According to https://learnersdictionary.com/qa/parts-of-the-day-early-morning-late-morning-etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morning=1, Afternoon=2, Evening=3, Night=4\n",
    "df_taxi['time session']=pd.cut(df_taxi['tpep_pickup_datetime'].dt.hour, [0,5,12,17,21,24],labels=[4,1,2,3,4],include_lowest=True, ordered=False)\n",
    "df_fhv['time session']=pd.cut(df_fhv['pickup_datetime'].dt.hour, [0,5,12,17,21,24],labels=[4,1,2,3,4],include_lowest=True, ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fhv['time session']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['date']= df_taxi['tpep_pickup_datetime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv['date']= df_fhv['pickup_datetime'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create date_num column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['date_num']pd.to_datetime(df_taxi[\"date\"]).dt.strftime(\"%Y%m%d\")\n",
    "df_fhv['date_num']pd.to_datetime(df_fhv[\"date\"]).dt.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hour column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['hour']= df_taxi['tpep_pickup_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv['hour']= df_fhv['pickup_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create day column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['day']= df_taxi['tpep_pickup_datetime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv['day']= df_fhv['pickup_datetime'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Numerical Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be looking at the Uber Dataset in terms of data cleaning numerical data since there is no numerical data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data= ['passenger_count', 'trip_distance', 'fare_amount',\n",
    "                 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "                 'improvement_surcharge', 'total_amount', \n",
    "                 'congestion_surcharge', 'total_trip_duration']\n",
    "\n",
    "df_taxi[numerical_data].describe().round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations based on testing df description:\n",
    "\n",
    "- Negative fare amount, extra, tip_amount, toll_amounts, mta_tax and total amount exists. This could be due to refunds and can be considered to be deleted from data. Also, fare amount has to be at least 2.50 USD (https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page)\n",
    "- Outlier->The maximum payments and distance seem to be excessive and large.\n",
    "- Outlier-> The minimum trip distances should be greater than 0\n",
    "- Median and mean is not always equal for certain attributes. This indicates that we are dealing with skewed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Invalid Relevant Payments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relevant process since we are considering to add tip_amount as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This removes the negative payments, free rides, no passengers and 0 distance rides\n",
    "df_taxi= df_taxi.loc[(df_taxi['fare_amount']>=2.5) &\n",
    "                   (df_taxi['trip_distance']> 0) &\n",
    "                    (df_taxi['tip_amount']>=0) &\n",
    "                    (df_taxi['passenger_count']>0)&\n",
    "                    (df_taxi['congestion_surcharge']> 0) & \n",
    "                    (df_taxi['total_trip_duration']> 1) &\n",
    "                     (df_taxi['trip_distance']< 613) &\n",
    "                    (df_taxi['total_trip_duration']< 12*60), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['fare_amount', 'trip_distance', 'congestion_surcharge']:\n",
    "    df_taxi.boxplot(column= i)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove isolated outliers and cut tip amount at 200 to remove extremely high tip amounts\n",
    "df_taxi= df_taxi.loc[(df_taxi['fare_amount']< 1000) & \n",
    "                   (df_taxi['trip_distance']< 200), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.netstate.com/states/geography/ny_geography.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv= df_fhv.loc[(df_fhv['total_trip_duration']> 1) &\n",
    "                    (df_fhv['total_trip_duration']< 12*60), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Passenger Count column for FHV Dataset\n",
    "We are assuming that the passenger count is under a similar distribution as the taxi dataset. This is further backed up by the report indicating that uber passenger count mean is 1.75-2 matching with the summary we have indicating that passenger count mean is 1.6. (https://pantonium.com/some-uber-statistics/) \n",
    "However, in order to determine whether we are to use the median (1) or mean (2), we should calculate the percentage of taxi dataset where there is 1 passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median and mean is not always equal for certain attributes. This indicates that we are dealing with skewed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df_taxi[df_taxi['passenger_count']==   1])/df_taxi.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since around 70% of the data is of one pessenger and since 1 is a minimum, the data is indicated to be right skewed. Hence , we will use the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv['passenger_count']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Location Zone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzone.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Instances from FHV data that are not avaible in taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_lst= list(df_taxi['PULocationID'].unique())\n",
    "loc_lst_fhv= list(df_fhv['PULocationID'].unique())\n",
    "print(\"Taxi Loc ID list:\", loc_lst)\n",
    "print(\"FHV Loc ID list:\", loc_lst_fhv)\n",
    "del_lst= []\n",
    "for i in loc_lst_fhv:\n",
    "    if i not in loc_lst:\n",
    "        del_lst.append(i)\n",
    "print(\"To be deleted:\", del_lst)\n",
    "for i in del_lst:\n",
    "    print(\"removing\",i,\"...\")\n",
    "    df_fhv= df_fhv.loc[df_fhv['PULocationID']!= i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is duplicates. This statement is:\", dfzone.duplicated(subset=['Borough', 'Zone', 'service_zone']).any())\n",
    "dfzone[dfzone[['Borough', 'Zone', 'service_zone']].duplicated(keep=False) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into the duplicates further before considering to remove the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Taxi datasets:\")\n",
    "#Checking Location ID 56 and 57\n",
    "print(\"The number of trips with PUlocation ID of 56 is\", len(df_taxi.loc[df_taxi['PULocationID']== 56]))\n",
    "print(\"The number of trips with PUlocation ID of 57 is\", len(df_taxi.loc[df_taxi['PULocationID']== 57]))\n",
    "#Checking location ID 103,104,105\n",
    "print(\"The number of trips with PUlocation ID of 103 is\", len(df_taxi.loc[df_taxi['PULocationID']== 103]))\n",
    "print(\"The number of trips with PUlocation ID of 104 is\", len(df_taxi.loc[df_taxi['PULocationID']== 104]))\n",
    "print(\"The number of trips with PUlocation ID of 105 is\", len(df_taxi.loc[df_taxi['PULocationID']== 105]))\n",
    "\n",
    "print(\"For FHV datasets:\")\n",
    "#Checking Location ID 56 and 57\n",
    "print(\"The number of trips with PUlocation ID of 56 is\", len(df_fhv.loc[df_fhv['PULocationID']== 56]))\n",
    "print(\"The number of trips with PUlocation ID of 57 is\", len(df_fhv.loc[df_fhv['PULocationID']== 57]))\n",
    "#Checking location ID 103,104,105\n",
    "print(\"The number of trips with PUlocation ID of 103 is\", len(df_fhv.loc[df_fhv['PULocationID']== 103]))\n",
    "print(\"The number of trips with PUlocation ID of 104 is\", len(df_fhv.loc[df_fhv['PULocationID']== 104]))\n",
    "print(\"The number of trips with PUlocation ID of 105 is\", len(df_fhv.loc[df_fhv['PULocationID']== 105]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only relatively much smaller proportion of trips in area 57 compared to 56, it is safe to assume that there may have been a mistake and the same location was matched to a locationID twice. Hence, all trips from locationid 57 will be marked as 56. As for the the last 3 locationsIDs, there is no need to combine data since Locations IDs 103 and 104 are not present in the taxi and fhv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi.loc[df_taxi['PULocationID']== 57,'PULocationID' ] = 56\n",
    "df_taxi.loc[df_taxi['DOLocationID']== 57, 'DOLocationID'] = 56\n",
    "df_fhv.loc[df_fhv['PULocationID']== 57, 'PULocationID'] = 56\n",
    "df_fhv.loc[df_fhv['DOLocationID']== 57, 'DOLocationID'] = 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Invalid Zones\n",
    "\n",
    "Remove the last two invalid entries and any other similar entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzone.loc[dfzone[\"Borough\"]=='Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi= df_taxi.loc[df_taxi['PULocationID']!= 264]\n",
    "df_fhv= df_fhv.loc[df_fhv['PULocationID']!= 264]\n",
    "df_taxi= df_taxi.loc[df_taxi['PULocationID']!= 265] \n",
    "df_fhv= df_fhv.loc[df_fhv['PULocationID']!= 265] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather = pd.read_csv(\"../data/large/weather_data_3-5_2019.csv\")\n",
    "dfweather.reset_index().to_feather('../data/large/weather_data_3-5_2019.feather')\n",
    "dfweather = pd.read_feather('../data/large/weather_data_3-5_2019.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code drops the unnecessary date, ename the columns to be more readable and change the data to the appropriate datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather= dfweather.rename(columns={\"valid\":\"datetime\",\"lon\": \"longitude\", \"lat\": \"latitude\", \"tmpf\": \"tempF\", \"relh\": \"relhumidity\", \"p01m\" : \"precipitation\" }, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather= dfweather.drop(columns=['index','station','tmpc', 'dwpf','dwpc', 'feel', 'drct', 'sknt', 'sped', 'alti', 'mslp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dates to  datetime type\n",
    "dfweather['datetime']= dfweather['datetime'].apply(lambda ogdate: datetime.strptime(str(ogdate), '%Y-%m-%d %H:%M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if there is missing entries\n",
    "print(dfweather.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather= dfweather.loc[dfweather['relhumidity'].notna()]\n",
    "dfweater= dfweather.loc[dfweather['precipitation'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfweather.describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude and latitude seems to be unnecessary information since all of are the same (-74longitude and 41 latitude). Hence it can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather= dfweather.drop(columns =['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t\n",
    "dfweather.loc[dfweather['precipitation']==\"T\", 'precipitation']= 0.005 #changed to 0.005 because meteoroligst counts min 0.01\n",
    "dfweather[\"precipitation\"] = dfweather[\"precipitation\"].apply(pd.to_numeric)\n",
    "dfweather.loc[dfweather[\"precipitation\"]>0.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather['time session']=pd.cut(dfweather['datetime'].dt.hour, [0,5,12,17,21,24],labels=[4,1,2,3,4],include_lowest=True, ordered=False)\n",
    "dfweather['date']= dfweather['datetime'].dt.date\n",
    "del dfweather['datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_taxi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi= df_taxi.rename(columns={\"tpep_pickup_datetime\":\"pickup_datetime\", \"tpep_dropoff_datetime\": \"dropoff_datetime\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taxi dataset has\", df_taxi.isnull().sum().sum(),\"missing values.\")\n",
    "print(\"FHV dataset has\", df_fhv.isnull().sum().sum(),\"missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taxi Dataset had originally\", old_size, \". Now, there is only\", len(df_taxi), \"elements in the dataset.\")\n",
    "print(\"FHV Dataset had originally\", old_size_fhv, \". Now, there is only\", len(df_fhv), \"elements in the dataset.\")\n",
    "print(\"Total trip data of \", len(df_taxi)+len(df_fhv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi.reset_index().to_feather(\"../preprocessed_data/feather/yellow_tripdata_2019.feather\")\n",
    "print(\"taxi file saved\")\n",
    "df_fhv.reset_index().to_feather(\"../preprocessed_data/feather/fhv_tripdata_2019.feather\")\n",
    "print(\"fhv file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfweather.reset_index().to_feather('../preprocessed_data/feather/dfweather.feather')\n",
    "print(\"weather file saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
