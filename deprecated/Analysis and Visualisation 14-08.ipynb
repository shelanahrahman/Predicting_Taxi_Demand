{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing- Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_classif,  f_classif\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import zipfile\n",
    "from pyproj import Proj, transform\n",
    "import geopandas as gpd\n",
    "# a nice way of filtering out deprecated warnings\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Taxi and FHV Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/lpgzfz8d1lv6zzph655b4mkm0000gn/T/ipykernel_16600/3549556410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainingdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../preprocessed_data/trainingdf.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestingdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../preprocessed_data/testingdf.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainingdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainingdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "trainingdf= pd.read_csv('../preprocessed_data/trainingdf.csv')\n",
    "testingdf= pd.read_csv('../preprocessed_data/testingdf.csv')\n",
    "\n",
    "X_train=trainingdf.iloc[:, 1:]\n",
    "y_train=trainingdf.iloc[:,0]\n",
    "X_test=testingdf.iloc[:,1:]\n",
    "y_test=testingdf.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open location zone information\n",
    "dfzone = pd.read_csv(\"../raw_data/taxi+_zone_lookup.csv\")\n",
    "# open location shapefile\n",
    "with zipfile.ZipFile(open(r'../data/large/taxi_zones.zip', 'rb')) as zip_ref:\n",
    "    zip_ref.extractall('../data/large/')\n",
    "sf = gpd.read_file(\"../data/large/taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert low,med,high demand to numerical bins\n",
    "y_train_num= y_train.replace({\"low\":1, \"med\":2, \"high\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the geometry shaape to to latitude and longitude\n",
    "# Please attribute this if you are using it\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tot= X_train.copy()\n",
    "train_tot['total demand']= y_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoJSON = gdf[['LocationID','geometry']].drop_duplicates('LocationID').to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on how to plot aggregated data.\n",
    "m.add_child(folium.Choropleth(\n",
    "    geo_data=geoJSON,\n",
    "    name='choropleth',\n",
    "))\n",
    "\n",
    "m.save('../plots/foliumChoroplethMap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# an example of what the geoJSON looks like\n",
    "json.loads(geoJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[['LocationID','total demand']].groupby('LocationID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','total demand'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name= 'Trip Demand (Discrete)' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapTrips.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf[['LocationID','trip_distance']].groupby('LocationID').sum().reset_index()\n",
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','trip_distance'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Trip Distances (miles)' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapTripdistance.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf[['LocationID','passenger_count']].groupby('LocationID').sum().reset_index()\n",
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','passenger_count'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Passenger Count' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapPassengerCount.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf[['LocationID','fare_amount']].groupby('LocationID').sum().reset_index()\n",
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','fare_amount'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Fare Amount (USD)' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapFareAmountTrips.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf[['LocationID','precipitation']].groupby('LocationID').sum().reset_index()\n",
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','precipitation'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Trips' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapWeatherTrips.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(train_tot, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "gdf[['LocationID','tempF']].groupby('LocationID').sum().reset_index()\n",
    "m_trip_distance = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "# refer to the folium documentations on more information on how to plot aggregated data.\n",
    "folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=gdf, # data source\n",
    "    columns=['LocationID','tempF'], # the columns required\n",
    "    key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "    fill_color='OrRd', # color scheme\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Temperature (F)' # legend title\n",
    ").add_to(m_trip_distance)\n",
    "\n",
    "m_trip_distance.save('../plots/foliumChoroplethMapWeatherTempTrips.html')\n",
    "m_trip_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Data over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([y_train,X_train],axis=1)\n",
    "data= pd.melt(data,id_vars=\"time session\",value_vars=\"trip demand\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"time session\"]=data[\"time session\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.groupby(['time session','value'], as_index=False).count()\n",
    "data= data.rename(columns={\"value\": \"Trip Demand\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.barplot(x=\"time session\", y= 'variable', hue=\"Trip Demand\", hue_order= ['low','med','high'], data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for over days\n",
    "data = pd.concat([y_train,X_train],axis=1)\n",
    "data= pd.melt(data,id_vars=\"day\",value_vars=\"trip demand\")\n",
    "data= data.groupby(['day','value'], as_index=False).count()\n",
    "data= data.rename(columns={\"value\": \"Trip Demand\"})\n",
    "sns.lineplot(x=\"day\", y= 'variable', hue=\"Trip Demand\", hue_order= ['low','med','high'], data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for over weekdays\n",
    "data = pd.concat([y_train,X_train],axis=1)\n",
    "data['weekday']=data['date'].dt.day_name()\n",
    "data= pd.melt(data,id_vars=\"weekday\",value_vars=\"trip demand\")\n",
    "data= data.groupby(['weekday','value'], as_index=False).count()\n",
    "data= data.rename(columns={\"value\": \"Trip Demand\"})\n",
    "sns.barplot(x=\"weekday\", y= 'variable', hue=\"Trip Demand\",\n",
    "            order= [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n",
    "            hue_order= ['low','med','high'], data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for over weekdays\n",
    "data = pd.concat([y_train,X_train],axis=1)\n",
    "len(data[\"hour\"])\n",
    "#data= pd.melt(data,id_vars=\"hour\",value_vars=\"trip demand\")\n",
    "#data= data.groupby(['hour','value'], as_index=False).count()\n",
    "#data= data.rename(columns={\"value\": \"Trip Demand\"})\n",
    "#sns.lineplot(x=\"hour\", y= 'variable', hue=\"Trip Demand\",\n",
    "#            hue_order= ['low','med','high'], data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Distributions of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot on boxplot\n",
    "\n",
    "#standardise data\n",
    "X_train_2= X_train.copy()\n",
    "X_train_2[\"date\"]= X_train[\"date\"].apply(lambda date: int(date.strftime('%Y%m%d')))\n",
    "X_train_2[\"time session\"]= X_train_2[\"time session\"].astype(\"int\")\n",
    "data_n_2 = (X_train_2 - X_train_2.mean()) / (X_train_2.std())            \n",
    "data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)\n",
    "#the melt function finds individual variable value in terms of the trip demand categories\n",
    "data= pd.melt(data,id_vars=\"trip demand\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"variable\", y=\"value\", hue=\"trip demand\", data=data, showfliers=False)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see that there could be a correlation between trip_distance and fare_amount but we can verify this later. We should remove the features that have a very small variance such as passenger_count, mta_tax and tolls_amount. We should also remove features that all the classes are basically the same such as extra. Interestingly, the PULocationID has a significantly different distribution corresponding to a different level of demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first ten features             # standardization\n",
    "data = pd.concat([y_train,data_n_2.iloc[:,11:]],axis=1)\n",
    "data= pd.melt(data,id_vars=\"trip demand\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"trip demand\", data=data, showfliers=False)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total_amount appears to have a correlation with  trip_distance and fare_amount which distributions can see in the plot above but we can verify this later. We should remove the features that have a very small variance such as precipitation and congestion_surcharge. We should also remove features that all the classes are basically the same such as hour. Temperature and relativehumidityis associated with having a similar distribution for the middle and high demand but a slightly different distribution for low demand. Interestingly, the DOLocationID also has a significantly different distribution corresponding to a different level of demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ten features             # standardization\n",
    "check_names= [\"passenger_count\", \"mta_tax\", \"tolls_amount\",\n",
    "                \"congestion_surcharge\", \"precipitation\"]\n",
    "for i in check_names:\n",
    "    new = data_n_2[i].copy()\n",
    "    data = pd.concat([y_train,new],axis=1)\n",
    "    data= pd.melt(data,id_vars=\"trip demand\",\n",
    "                        var_name=\"features\",\n",
    "                        value_name='value')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.boxplot(x=\"features\", y=\"value\", hue=\"trip demand\", data=data, showfliers=False)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= X_train_2.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY STRONG Correlations between\n",
    "date and hour,\n",
    "fare amount, trip distance, and total amount\n",
    "\n",
    "\n",
    "Medium Correlation between DOLocationID and PULocationID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train.drop(columns= [\"passenger_count\", \"mta_tax\", \"tolls_amount\",\n",
    "                              \"congestion_surcharge\", \"extra\", \"hour\", \"fare_amount\",\n",
    "                              \"trip_distance\", \"date\", \"precipitation\", \"relhumidity\"])\n",
    "\n",
    "X_test_reduced = X_test.drop(columns= [\"passenger_count\", \"mta_tax\", \"tolls_amount\",\n",
    "                              \"congestion_surcharge\", \"extra\", \"hour\", \"fare_amount\",\n",
    "                              \"trip_distance\", \"date\", \"precipitation\", \"relhumidity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = X_train.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= pd.concat([y_train,X_train_reduced],axis=1)\n",
    "test_df= pd.concat([y_test,X_test_reduced],axis=1)\n",
    "train_df.to_csv('../preprocessed_data/trainingdf.csv', index=False)\n",
    "test_df.to_csv('../preprocessed_data/testingdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(random_state=43)      \n",
    "clr_rf = clf_rf.fit(X_train_reduced,y_train)\n",
    "\n",
    "ac = accuracy_score(y_train,clf_rf.predict(X_train_reduced))\n",
    "print('Testing accuracy is: ',ac)\n",
    "\n",
    "ac = accuracy_score(y_test,clf_rf.predict(X_test_reduced))\n",
    "print('Training accuracy is: ',ac)\n",
    "\n",
    "#cm = confusion_matrix(y_test,clf_rf.predict(X_test_reduced))\n",
    "#sns.heatmap(cm,annot=True,fmt=\"d\",yticklabels=True)\n",
    "plot_confusion_matrix(clf_rf, X_test_reduced, y_test,\n",
    "                                 display_labels= ['Low', \"mid\",\"high\"],\n",
    "                                 cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
